The primary objective of this thesis is to assess the potentials and limitations of our APR pipeline when integrated into a real-world software development lifecycle. We aim to answer the following research question to evaluate the system's capabilities and impact on the software development process: \\
\textbf{What are the potentials and limitations of integrating an LLM-based automated bug-fixing pipeline into a CI/CD workflow, as measured by repair success rate, execution times, and cost, and how does this integration impact the overall software development lifecycle?}

For answering this question we streamlined this process into three phases Preparation, Implementation / Application and Evaluation.

% \includegraphics{images/}


\section{Preparation}
For implementing and evaluating our system we first need to prepare an environment where the system can be applied. This includes selecting a suitable dataset, setting up the environment, and specifying the requirements for the system.
\subsection{Dataset Selection}
For the evaluation of our APR integration, we selected the QuixBugs benchmark \cite{linQuixBugsMultilingualProgram2017}. This dataset is well-suited for our purposes due to its focus on small-scale software bugs in Python. It consists of 40 individual files containing an algorithmic bug. A bug is always a single erroneous line in a file. For every file there is a corresponding tests for repair validation. Since these bugs where developed as challenging problems for developers \cite{linQuixBugsMultilingualProgram2017},it enables us to evaluate if our system can take over the complex fixing of small bugs without developer intervention to prevent context switching for developers.

Compared to other APR benchmarks like SWE-Bench \cite{jimenezSWEbenchCanLanguage2024} QuixBugs is relatively small which accelerates setup and development.

if achieved I will ad swe bench lite later \cite{jimenezSWEbenchCanLanguage2024}

\subsection{Environment Setup}
To mirror realistic software development environment, we prepared a GitHub repository containing the QuixBugs dataset. This repository serves as the basis for the bug fixing process, allowing the system to interact with the codebase and perform repairs. The repository contains only relevant files and folders required for the bug fixing process, ensuring a clean environment for the system to operate in.

Using the relevant files we generated a GitHub issue for each bug, using a consistent template that captures only the title of the Problem. These issues serve as the entry points to our APR pipeline.
---IMAGE of ISSUE

\subsection{Requirements Specification}
Before implementation we constructed functional and non functional requirements to measure process and document the process further. The requirements are detailed in \ref{Requirements}.

\section{Pipeline Implementation}
The Automated Bug Fixing Pipeline was developed using iterative prototyping and testing, with a focus on simplicity and modularity. Using the self developed requirements \ref{Requirements} we build the following System:

---IMAGE of HIGH LEVEL System Architecture here

When the system is in place in a target repository. An issue with the default or configured bug label can be created. This trigger trigger will spin up a github action runner which executes the APR pipeline. This Pipeline takes to the configured LLM Api (google and openai) to localize and fix the bugy files. With the supplied file edits the code is validated and tested. When validation passes the a pull request with the file changes is automatically opened on the repository, linking the issue and providing details about the repair process. In case of a unsuccessful repair the failure is reported to the issue.


\section{Evaluation}

In this section, we describe how we measure the effectiveness and performance of our APR pipeline when integrated into a real-world CI process, using our QuixBugs repository as a bases.

For Evaluation we will focus on several key metrics to assess the system's performance and abilities in repairing software bugs. These metrics will provide insights into the system's efficiency, reliability, and overall impact on the software development lifecycle. The following metrics are automatically collected and calculated for each run of the APR pipeline:

% TODO averages for all bugs on each model and compare single issue runs with multiple issue runs - also for with and without multi attempt and feedback
\begin{itemize}
    \item\textbf{Repair Success Rate:} Calculate the percentage of successfully repaired bugs out of the total number of bugs attempted by using test results.
A successful repair is defined as a bug passing all tests associated with it.


\item\textbf{Number of Attempts:} Track the number of attempts made by the system to repair each bug with a maximum of 3 attempts.


\item\textbf{Overall Execution Time in CI/CD:} Evaluate the time taken for the system to execute within a CI/CD pipeline, providing insights into its performance in real-world development environments.


% TODO should I split overall execution and stages into separate metrics?
\item\textbf{Execution Times of Dockerized Agent:} Measure the time taken by the Containerized agent and its stages to execute the repair process and the  execution times of individual stages.

With this CICD overhead can be calculated and bottlenecks can be identified


\item\textbf{Token Usage}: Monitor the number of tokens used by the LLM during the repair process, which can help understanding the cost of repairing and issue and the relation between token usage and repair success.


\item\textbf{Cost per Issue:} Calculate the cost associated with repairing each bug, considering factors such as resource usage, execution time, and any additional overhead.
\end{itemize}

% TODO
explain how these metrics are collected and calculated, including any tools or scripts used to automate the process.
explain how repair success rate is determined

% TODO
ask zhang wether i need to include the evaluation of swe bench lite from the paper or if a ref is enough

% TODO
explain statistical methods used to analyze the collected data, such as averages, medians, and standard deviations. This will help in understanding the variability and reliability of the results.

% TODO 
explain how resuLTS are calculated for model comparison

what I evaluate:
script execution time + CICD overhead
one issue vs multiple issue times
model vs model metrics
costs
attempts vs no attempts in all these categories