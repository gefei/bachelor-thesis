In the following section we will present our results our implementation and evaluation. We implemented a working prototype for accessing how to integrating LLM based Automated Bug Fixing into Continuous Integration and the resulting potentials and limitations of using this system in software development workflows.

The setup and usage of the prototype in a GitHub repository is demonstrated in the first part of this chapter. By showcasing the resulting workflow in the GitHub Web user interface. In the second part of this section we  present the results of the quantitative evaluation of the prototype being used on the repository containing the QuixBugs dataset as a bases.

\section{Showcase of workflow} \label{section:showcase}

Setting up the APR system in a repository is archived by adding 2 files to the ``.github'' directory of the repository. As seen in \ref{fig:setup} the required files are the ``.github/workflows/auto-fix.yml'' and ``.github/scripts/filter\_issues.py''. With this the system is ready to operate. Additionally a optional configuration file can be added to the root, this ``.bugfix.yml'' file can be used to overwrite the LLM model used, max attempts and naming conventions of labels and branches. Furthermore a ``LLM\_API\_KEY'' secret needs to be added to the repository secrets and GitHub actions needs to be granted permission to create Pull Requests.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=1\textwidth]{images/workflow/setup.png}
%     \caption{Set up of APR in a repository}
%     \label{fig:setup}
% \end{figure}

With the system in place and a custom configuration file set up, the APR system is ready to be used in the repository. Bugs can be automated fixed in two different ways.

Processing a single issue right away when the issue is created and labeled with the ``bug\_v01'' shown in Figure \ref{fig:issue-trigger}. This allows for fast feedback and quick bug fixing at issue creation and triage \footnote{explain triage}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/workflow/label_issue.png}
    \caption{Trigger automatic fixing for single issue}
    \label{fig:issue-trigger}
\end{figure}

The second way collects and processes all issues labeled with the ``bug\_v01'' label by scheduling the workflow to run at a specific time or dispatching it manually (see Figure \ref{fig:dispatch}). This allows for a more controlled approach for bug fixing, where issues are processed in batches.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/workflow/manual_dispatch.png}
    \caption{Manual Dispatch of APR}
    \label{fig:dispatch}
\end{figure}

When the workflow is triggered it creates a new run in the GitHub Actions tab (see figure \ref{fig:apr-action}). This executes the bug fixing logic described in \ref{chapter:implementation}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/workflow/new_action.png}
    \caption{GitHub Action Run}
    \label{fig:apr-action}
\end{figure}

A run can produce two possible outcomes for each issue it processes. Firstly, on a successful repair attempt it creates a Pull Request with the changes made to the codebase and the issue linked to it. The created Pull Requests allow code review and merging of fixes changes into the main branch. When merged the issue will automatically closed. Figure \ref{fig:pr} shows an example of a resulting Pull Request created by the APR system.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/workflow/pr.png}
    \caption{Resulting Pull Request}
    \label{fig:pr}
\end{figure}

Secondly when an issue repair fails after all attempts have been exhausted, the failure is reported to the issue as a comment and the issues is labeled as failed (see figure \ref{fig:failure-report}) so it wont be picked up again. This allows for easy tracking of issues that could not be fixed by the APR system.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/workflow/failure_comment.png}
    \caption{Failure Report}
    \label{fig:failure-report}
\end{figure}

Failed issues can be picked up again by adding new context to the issue. This can be done by adding a comment to the issue, which will trigger the APR system to pick up the issue again and try to fix it with the new context. This allows for a more dynamic approach to bug fixing, where issues can be fixed as new information becomes available.

%TODO pipeline from comment screenshot

For transparency and debugging, each run provides a live log stream in the GitHub Actions tab (see figure \ref{fig:log-stream}). This allows users to see the progress of the run and any errors that occur during the execution. For further analysis logs, metrics and the complete context are published as artifacts to each run available to download in the run view.
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{images/workflow/logs.png}
    \caption{APR log stream}
    \label{fig:log-stream}
\end{figure}


Figure \ref{fig:flow} visualized the resulting flow of the APR system. The diagram shows the relation between user actions (COLOR), repair success (COLOR) and the steps taken by the system.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{images/flowcharts/flowresult.png}
    \caption{Resulting flow diagram}
    \label{fig:flow}
\end{figure}

\section{Evaluation Results} \label{section:evaluation-results}

In this section we present the results of the quantitative evaluation of the implemented APR prototype. This evaluation is done using the collected and calculated data for each run of the prototype. How this data was collected and calculated is described in \ref{table:run-metrics}.

\subsection{Validity}
- github runners have a lot of computational noise
- only small set of runs.

We asses more threats to validity in section \ref{section:validity}.

\subsection{Baseline of Evaluation}
The resulting data is based on the executions of the APR prototype in the prepared repository (see \ref{subsection:environment-setup}) which contains all 40 issues from the QuixBugs benchmark. For evaluating the effectiveness, performance and cost we ran the APR prototype using eleven selected LLMs models defined in \ref{subsection:llm-selection}. All models where tested with one attempt per issue to evaluate zero shot performance and with a retry loop enabled to compare a few shot performance.
\subsection{Results}
The following tables tables show the repair success rate, average cost per issue and average execution time per issue for each model used in the evaluation. The first table \ref{table:results} shows the results of the evaluation with one attempt per issue, while the second table \ref{table:retry-results} shows the results with the retry loop enabled and max attempts set to 3.

%TODO add exact model snapshot versions
\begin{table}[H]
    \centering
    \small
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} p{3.5cm} | p{3cm} | p{3cm} | p{3cm} @{}}
        \hline
        \textbf{Model} & \textbf{Repair Success Rate} & \textbf{Average Cost Per Issue} & \textbf{Average Execution Time in seconds} \\
        \hline
        \textbf{gemini-2.0-flash-lite} & 87.5\% & \$0.0001 & 11.33 \\
        \textbf{gemini-2.0-flash} & 72.5\% & \$0.0002 & 10.81s \\
        \textbf{gemini-2.5-flash-lite} & 87.5\% & \$0.0002 & 6.36s \\
        \textbf{gemini-2.5-flash} & 90.0\% & \$0.009 & 50.16s \\
        \textbf{gemini-2.5-pro} & 95.0\% & \$0.0713 & 70.09s \\
        \textbf{gpt-4.1-nano} & 62.5\% & \$0.0002 & 14.88s \\
        \textbf{gpt-4.1-mini} & 92.5\% & \$0.0006 & 18.75s \\
        \textbf{gpt-4.1} & 92.5\% & \$0.0033 & 12.93s  \\
        \textbf{o4-mini} & 87.5\% & \$0.0064 & 57.05s  \\
        \textbf{claude-3-5-haiku} & 2.5\% & \$0.0024 & 23.1s \\
        \textbf{claude-3-7-sonnet} & 87.5\% & \$0.0069 & 21.78s \\
        \textbf{claude-sonnet-4-0} & 92.5\% & \$0.0102 & 28.93s \\
        \hline
    \end{tabular*}
    \caption{Zero shot evaluation results}
    \label{table:results}
\end{table}

\begin{table}[H]
    \centering
    \small
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} p{3.5cm} | p{3cm} | p{3cm} | p{3cm}  @{}}
        \hline
        \textbf{Model} & \textbf{Repair Success Rate} & \textbf{Average Cost}  & \textbf{Average Execution Time} \\
        \hline
        \textbf{gemini-2.0-flash-lite} & 82.5\% & \$0.0002  & 19.97s \\
        \textbf{gemini-2.0-flash} & 82.5\% & \$0.0004  & 15.62s \\
        \textbf{gemini-2.5-flash-lite} & 87.5\% & \$0.0003 & 10.80s \\
        \textbf{gemini-2.5-flash} & 92.5\% & \$0.01362  & 71.37s \\
        \textbf{gemini-2.5-pro} & 100\% & 0.00  & 0.00s \\
        \textbf{gpt-4.1-nano} & 70.0\% & \$0.0003  & 34.85s \\
        \textbf{gpt-4.1-mini} & 97.5\% & \$0.0043  & 24.42s \\
        \textbf{gpt-4.1} & 97.5\% & 0.0041  & 14.76s \\
        \textbf{o4-mini} & 100\% & 0.0075  & 60.98s \\
        \textbf{claude-3-5-haiku} & 15.0\% & \$0.0075  & 65.18s \\
        \textbf{claude-3-7-sonnet} & 95.0\% & \$0.0087 & 24.39s \\
        \textbf{claude-sonnet-4-0} & 92.5\% & \$0.011 & 30.13s \\
        \hline
    \end{tabular*}
    \caption{Few shot evaluation results}
    \label{table:retry-results}
\end{table}

The full set of resulting data can be found the in ``apr\_evaluation'' directiory of the quixbugs-apr repository, listed in the Appendix \ref{Appendix}.