In the following section we will showcase the resulting workflow of our prototype and the evaluation results for the Quixbugs benchmark.
\section{Showcase of workflow}
To integrate the APR system into a repository living on GitHub we need to move the pipeline with its filter script to the dedicated github action workflow directory.
---IMAGE OF WORKFLOW IN PLACE

When the workflow is in place the APR system is ready to go. Optionally its default behavior can be altered by adding a configuration file (called: bugfix.yml) to the root of the repository.
---EXAMPLE OF CONFIG

Now when an issue is created and labeled with the default label "bug" (or a custom label defined in the configuration file) the APR system will be triggered and start the bug fixing process. Manual triggering is also possible by using the "Run workflow" button in the GitHub actions tab of the repository.
---IMAGE OF ISSUE BEING CREATED OR MANUALLY TRIGGERED

After the workflow is triggered and relevant issues are found the APR system start as a run of the GitHub action workflow.
---IMAGE OF RUN BEING STARTED


When the automatic bug fixing process has completed there are two possible outcomes:
Pull Request with patch for bug and link to the issue.
---IMAGE OF PULL REQUEST

or a comment on the issue that bug fixing failed after all attempts.
---IMAGE OF COMMENT ON ISSUE

After the Workflow finishes metrics and logs are available for download in the action.
(during a run logs are live streamed in the Workflow run)
---IMAGE OF LOGS

\section{Evaluation Results}
Our evaluation is based on the data collected during the execution of the prototype. This information is stored in the artifacts of the Github Actions run. Using the \textbf{get\_run\_data.py} script we collected the APR artifacts and the GitHub Pipeline information using the GitHub API. The script then processes the data and generates a report with the results of the evaluation. With the fetched data we calculate the metrics defined in %\ref{Evaluation}.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} p{2cm} p{2cm} p{7cm} p{4cm} @{}}
        \toprule
        \textbf{Model} & \textbf{Languages} & \textbf{Description} & \textbf{Difficulty} \\
        \midrule
        Quixbugs & Python, Java & 40 small single line bugs in Python code  & Easy \\[6pt]
        Defects4J & Java & real-world Java bugs & Medium \\[6pt]
        SWE Bench & Python, JavaScript & Real GitHub defects in software engineering repositories & Hard \\[6pt]
        \bottomrule
    \end{tabular*}
    \caption{Functional requirements (F0--F8)}
\end{table}

Single Issue Processing:
CI overhead

Multi Issue Processing:
CI overhead

Also include attempt loop

with small models and attempt loop makes small models pass the whole benchmark?