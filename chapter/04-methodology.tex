\section{Preparation}
\subsection{Dataset Selection}
For the evaluation of the APR integration into the software development lifecycle, we selected the Quixbugs dataset \cite{linQuixBugsMultilingualProgram2017} as our primary benchmark for testing APR integration. This dataset is well-suited for our purposes due to its focus on small-scale software bugs in Python. It consists of 40 algortithmic bugs each in one file consisting of a single erroinums line , each with a correspoding tests for repair validation. Because these bugs where developed as challending problems for developers \cite{linQuixBugsMultilingualProgram2017}, we can evaluate if our system can take over the complex fixing of small bugs without developer intervention to prevent context switching for developers.

Compared to other APR benchmarks like SWE-Bench \cite{jimenezSWEbenchCanLanguage2024} Quixbugs is relaivly small which accelerates setup and development.

if archieved I will ad swe bench lite later \cite{jimenezSWEbenchCanLanguage2024}

\subsection{Dataset Preparation / setup}
To mirrow realistic software development environment, we prepared the Quixbugs dataset by creating a GitHub repository. This repository serves as the basis for the bug fixing process, allowing the system to interact with the codebase and perform repairs. The repository contains only relevant files and folders required for the bug fixing process, ensuring a clean environment for the system to operate in.

We automatically generated a GitHub issue for each bug, using a consistent template that captures just the Title of the Problem. These issues serve as the entry points to our APR pipeline.

%TODO remove repetitive mentioneing of effectiveness and integration of the system
\section{Evaluation Stragegy and Metrics}

metrics to evaluate the system's performance, effectiveness in repairing software bugs and integration into a real world software developemnt lifecycle are crucial for understanding its impact and areas for improvement. 

\subsection{Evaluation Strategy}
The evaluation strategy will involve the following steps:
1. **Dataset Preparation**: Use the QuixBugs dataset, which contains a variety of small-scale software bugs in Python, to test the system's repair capabilities.
2. **System Configuration - Process Review**: Set up the system in a controlled environment, ensuring that it integrates into a real world sfotware development envrioment. Having acess to nesseary resources and tools for the repair process.
3. **Execution of Repair Process**: Run the system through the bug fixing lifecycle, from issue creation to pull request generation, while monitoring its performance.
5. **Automatic Evaluation**: During a the repair process the tests are executed to validate the repairs made by the system. The system will automatically evaluate the success of each repair attempt based on the test results and report it. (for swe bench just evalutation -> results)
4. **Data Collection**: Collect data on the system's performance during the repair process, including execution time, success rates, and any encountered issues. Metrics listed \ref{Metrics}.
5. **Analysis of Results**: Analyze the collected data to evaluate the system's bug repairing capabilities and integration, focusing on both quantitative metrics and qualitative feedback from the repair process. +manual inspection / comapre with correct solution
\subsection{Metrics}
For Evaluation we will focus on several key metrics to assess the system's performance and abilites in repairing software bugs. These metrics will provide insights into the system's efficiency, reliability, and overall impact on the software development lifecycle. The following metrics will be used:
- **Overall Execution Time in CI/CD**: Evaluate the time taken for the system to execute within a CI/CD pipeline, providing insights into its performance in real-world development environments.
- **Execution Time of Docker Agent**: Measure the time taken by the Containerized agent to execute the repair process, which helps in understanding the efficiency of the containerized environment and help evaluate the computer overhead of CICD.
- **Repair Success Rate**: Calculate the percentage of successfully repaired bugs out of the total number of bugs attempted by using test results.
- **Number of Attempts**: Track the number of attempts made by the system to repair each bug.
- **Number of Code Issues**: Count the total number of code issues identified and attempted to be repaired by the system.
- **Cost per Issue**: Calculate the cost associated with repairing each bug, considering factors such as resource usage, execution time, and any additional overhead.



- Metrics:
- Full Execution Time in CI/CD
- Execution Time of docker agent
- Execution Time per Stage
- Repair Success Rate
- nubmer of Attempts
- Cost per issue
- Token Usage




what I evaluate:
script execution time + CICD overhead
one issue vs mutliple issue times
model vs model metrics 
costs
attemps vs no attemps in all these categories