The primary objective of this evaluation is to assess the potentials and limitations of our APR pipeline when integrated into a real-world software development lifecycle. We aim to answer the following research question (RQs) to evaluate the system's capabilities and impact on the software development process:
RQ1: Which potentials and limitations does the APR pipeline have ???

\section{Preparation}
\subsection{Dataset Selection}
For the evaluation of the APR integration into the software development lifecycle, we selected the Quixbugs dataset \cite{linQuixBugsMultilingualProgram2017} as our primary benchmark for testing APR integration. This dataset is well-suited for our purposes due to its focus on small-scale software bugs in Python. It consists of 40 algortithmic bugs each in one file consisting of a single erroinums line , each with a correspoding tests for repair validation. Because these bugs where developed as challending problems for developers \cite{linQuixBugsMultilingualProgram2017}, we can evaluate if our system can take over the complex fixing of small bugs without developer intervention to prevent context switching for developers.

Compared to other APR benchmarks like SWE-Bench \cite{jimenezSWEbenchCanLanguage2024} Quixbugs is relaivly small which accelerates setup and development.

if archieved I will ad swe bench lite later \cite{jimenezSWEbenchCanLanguage2024}

\subsection{Environment Setup}
To mirrow realistic software development environment, we prepared the Quixbugs dataset by creating a GitHub repository. This repository serves as the basis for the bug fixing process, allowing the system to interact with the codebase and perform repairs. The repository contains only relevant files and folders required for the bug fixing process, ensuring a clean environment for the system to operate in.

We automatically generated a GitHub issue for each bug, using a consistent template that captures just the Title of the Problem. These issues serve as the entry points to our APR pipeline.

%TODO remove repetitive mentioneing of effectiveness and integration of the system

\subsection{Pipeline Arcitecture}


\section{Evaluation}

In this section, we describe how we measure the effectiveness and performance of our APR pipeline when integrated into a real-world CI process, using our QuixBugs repository as

For Evaluation we will focus on several key metrics to assess the system's performance and abilites in repairing software bugs. These metrics will provide insights into the system's efficiency, reliability, and overall impact on the software development lifecycle. The following metrics will be used:

The follwing metrics are automatically collected for each run of the APR pipeline:

- **Repair Success Rate**: Calculate the percentage of successfully repaired bugs out of the total number of bugs attempted by using test results.
- **Number of Attempts**: Track the number of attempts made by the system to repair each bug.
- **Overall Execution Time in CI/CD**: Evaluate the time taken for the system to execute within a CI/CD pipeline, providing insights into its performance in real-world development environments.
- **Execution Time of Dockerized Agent**: Measure the time taken by the Containerized agent to execute the repair process, which helps in understanding the efficiency of the containerized environment and help evaluate the computer overhead of CICD.
- ** Token Usage**: Monitor the number of tokens used by the LLM during the repair process, which can help in understanding the cost and efficiency of the model's usage.
- **Cost per Issue**: Calculate the cost associated with repairing each bug, considering factors such as resource usage, execution time, and any additional overhead.



what I evaluate:
script execution time + CICD overhead
one issue vs mutliple issue times
model vs model metrics
costs
attemps vs no attemps in all these categories