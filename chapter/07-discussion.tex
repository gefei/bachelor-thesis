In this section, we will discuss the results of the evaluation of our prototype and put it into context to answer the research questions. First evaluating the validity of our findings, the potential of our approach, its limitations, and summarize the lessons learned. Finally, we will outline a roadmap for future extensions of our work.

\section{Validity} \label{section:validity}
The results from the evaluation are very promising but still face some limitations to the validity of the results.

Because the repair process is based on LLMs, which are non-deterministic by design, executions can vary in their results. Since the pipeline way only run once per model these results are not fully representative of the models performance. Furthermore speed and availability of the tested LLMs is highly dependant on the providers APIs which can account for varying execution times or even failures during high traffic times. Costs are calculated based on token usage reported by the providers API responses, unfortunately this is not accurate as the providers are intransparent with actual token counts \cite{sunCoInCountingInvisible2025d}. This means that the reported costs and execution times are not fully accurate across providers and should be interpreted with caution.
In addition, the system was executed on GitHub provided GitHub Actions runners included in the GitHub free Limits. Therefore the performance metrics reflect GitHubs cloud-hosted CI environments. While allowing quick iterations and setup this limits the feasibility of absolute, execution times and costs.

The evaluation is based on the QuixBugs benchmark which consists of 40 single line issues each in a separate file. This dataset is not fully representative of real-world software development, as it only covers a small niche area of the complexity and variety of bugs that can arise in larger codebases. Moreover we assume that the tests show reliable correctness of a tested program. Although QuixBugs provides extensive tests for the size of the programs, these do not guarantee full behavioral equivalence with the ground truth. Consequently, a generated fix may pass the tests while being semantically incorrect.

We partially offset these limitations by evaluating against twelve diverse LLMs that are likely to translate to larger datasets and other CI platforms. But testing on larger benchmarks like Defects4J and SWE-Bench are required before drawing conclusions about real-world effectiveness.

The threats outlined above delimit the claim for problems outside the scope. Further testing on other benchmarks and programming languages are necessary to fully validate the approach in production scale settings. Nevertheless, the results demonstrate that an LLM based automated bug fixing pipeline can be integrated into a CI workflow and achieve non-trivial repair rates at with minimal time effort and low cost.

--add that knowledge cutoff is after the benchmark was publishes since the benchmark is open source it might have been in the training dataset of the models

\section{Potentials}

Section \ref{chapter:implementation} answers RQ1 by demonstrating how LLM based Automated Bug Fixing can be integrated into a CI workflow using GitHub Actions and containerized APR logic. A key advantage of this approach is, it allows for adjustments and further improvement without the need for major changes to the system.
The concept is applicable to other Python repositories but was only tested on the ``quixbugs-apr'' repo and the ``bugfix-ci'' development repository. The option for custom configuration makes it adaptable to different repositories and environments.

Section \ref{section:showcase} demonstrated that with this integration getting a automatically generated fix for a bug only requires a single user action. Creating and labeling an issue in the GitHub repository. This shows that the approach can take over and submit fixes, without the need for developer intervention. Creating issues/ticket for features, adjustments or bugs is a key part in agile frameworks for tracking tasks for in each iteration. %\cite{} 
The results indicate that this integration can support developers in an agile lifecycle by automating the design, development and test stages for a bug, leaving only requirement specification in form of issues and review of generated fixes to the developers. Furthermore issue descriptions for such bugs can be minimal, as the issues for evaluation contained no detailed information on the bug. This allows developers to focus on more complex tasks, while the system takes care of the simpler bugs. Our results align with the findings of \cite{houLargeLanguageModels2024}, who states that LLMs can accelerate bug fixing and enhance software reliability and maintainability.

The evaluation results in section \ref{section:evaluation-results} show that the prototype can achieve a repair success rate of up to 100\% on the QuixBugs benchmark with the best performing models (X, Y, Z). This indicates that the approach taken can be effective in fixing single file bugs in a CI environment. When taking a deeper look at the results, we observed that the repair success rate is highly dependent on the LLM model used.

With one attempt for every issue\footnote{zero shot prompting} the best performing models already achieve a repair success rate of 100\% on the QuixBugs benchmark, while smaller models like gemini-2.5-flash-lite and gpt-4.1-mini achieve a repair success rate of 95\% and 90\% respectively. This shows that with zero shot smaller models can achieve good results, but larger models are more effective in fixing bugs.

The potential of smaller models like: X,Y,Z lies in their performance and costs effectiveness. The results show that smaller models can achieve a repair success rate of 95\% with a significantly lower cost and execution time compared to larger models. For example X solves x\% with an average cost of X taking an average time of X per issue while Y repairs Y\% successful with only \$Y average per issue and 1/3 of the time per issue. This indicates that smaller models can be used to automate bug fixing in a CI environments, keeping costs low and performance resulting in quicker submissions of fixes.

The introduction of multi attempt fixes with an internal feedback loop, enhances the effectiveness of the integration significantly for every single model tested. By allowing the LLM to retry fix generation with additional context on failure, the repair success rates climb an average of X\% per model. Particularly smaller models benefit from the few shot approach because the success rates rise to factor of X while costs and execution times rise linearly. This shows that small models can archive similar performance to larger model with lower costs and better performance.

With average repair times reaching from x-Y per issue, the approach is feasible for use in a CI environment. Furthermore these times do not impose significant waiting times for developers and indicate that bugs can be fixed quickly and efficiently while developers focus on more important and complex tasks. With costs reaching from \$X to \$Y per issue, this approach is also relatively cost effective and affordable.

--zero shot inexpensive for bigger models while for smaller it makes sense for multi attempt

Overall repair success rates, execution times and costs demonstrate that the approach is feasible and can be used to automate bug fixing in a CI environment.
does not take significant time and furthermore it can be run concurrently

Leveraging paradigms of an agentless approach added with interactivity of the GitHub project platform we could achieve results that hold up with current APR research showing similar repair success rates on the QuixBugs benchmark while providing an integrated approach with transparent cost and performance insights. -- add which ones, cano1killbugs, ...

As mentioned before the performance is highly dependant on the underlying LLMs used. As companies like OpenAI, Google and Anthropic are constantly improving their models, the approach taken in this thesis can be extended to use future models as they become available. This means that the system can be improved over time without the need for major changes making the approach future proof and adaptable to new developments in the field of LLMs. The portability, modularity and extendability allows for future opportunities mentioned in \ref{section:roadmap}.

\section{Limitations}
Ultimately, there are also limitations faced by the prototype and the approach taken in general.

As mentioned in section \ref{section:validity}, the system is constrained to addressing small issues only. Even with small issues the timings and availability of the system is highly dependant on external dependencies like the LLM providers APIs and the provided GitHub Actions runners. Which makes this a limitation in terms of reliability and performance in a real software development cycle.

In terms of the integration with GitHub is that the system faces more limitations that come from the Github Actions environment. Runs can not be skipped and filtering logic of events with external configuration data is very limited. This results in the workflow having to execute on every issue labeling event to filter in the first job. This fills the GitHub Actions tab with runs that have success as status but do not have any bugs to fix, resulting in cluttered the run history which may lead to confusion for users. This could be solved by migrating the APR core to a GitHub App which listens to webhook events and only triggers the workflow for relevant issues.

Additionally security and privacy concerns arise from the fact that the program repair is based on LLMs. Since issue title and description get added to the prompt that is used for repair, malicious instructions could be put into an issue. Therefore non trusted project contributors should not be able to create or edit issues. Furthermore code submitted for fixing the bug as a pull request needs to be verified and reviewed carefully before merging.

Large LLM providers like Google, OpenAI and Anthropic are not fully transparent about their data and storage policies. This may raise concerns about the privacy of the code and issues processed by the system, especially for private or sensitive repositories. The prototype was not tested using open source models, but is by design modular and  extendable for the use of open source model. Nevertheless, copyright and licensing issues may arise when code is generated by LLMs that are based on copyrighted training data. \cite{sauvolaFutureSoftwareDevelopment2024, houLargeLanguageModels2024}


\section{Lessons Learned}
The development and evaluation of the prototype was an interesting and insightful experience. The following lessons were learned during the process:
- LLMs can be used to automate bug fixing in a CI environment, but the results are highly dependent on the quality of the LLMs.
- The apis of llm provders can be unreliable espically googles gemini- 2.5-pro.
- The field of LLMs is rapidly evolving, and new models are released frequently. This makes it difficult to keep up with the latest developments.


\section{Roadmap for Extensions} \label{section:roadmap}
As mentioned before the prototype developed in this thesis is modular and easily extendable. This allows for future opportunities and research. Below we list potentials for further extensions and analysis which was not implemented because of the contained time for thesis.

- cost, time and repair success could be optimized
- test different prompts
- richer benchmarks
- Service Accounts for better and more transparent integration
- try out complex agent architectures and compare metrics and results
- try out more complex bug fixing tasks - SWE bench
- concurrency and parallelization of tasks
- app which replies on webhook events
- measure developer trust and satisfaction
- dig further into collected data, see where it went wrong in the patches or localization
- small model with fallback to larger model
- more complex prompts and prompt engineering
- token usage can be optimized
- further analyze the collected data, we collected a lot of data which can be used for more insights