In this section, we will discuss the results of the evaluation of our prototype and put them into context to answer the research questions. We begin by evaluating the validity of our findings, the potential of our approach, and its limitations, and summarize the lessons learned. Finally, we outline a roadmap for future extensions of our work.

\section{Validity} \label{section:validity}

The results from the evaluation are very promising but face several limitations that affect their validity.

Since the repair process relies on \acp{LLM}, which are non-deterministic by design, executions may vary in their results. As the pipeline was only run once per model, these results are not fully representative of each model's performance. Furthermore, the speed and availability of the tested \acp{LLM} heavily depend on the providers' APIs, which can cause fluctuating execution times or even failures during periods of high traffic.

Costs are calculated based on token usage reported by the providers' API responses. However, these figures are not fully accurate, as providers are typically non-transparent about actual token counts \cite{sunCoInCountingInvisible2025d}. As a result, the reported costs and execution times should be interpreted with caution and are not reliably comparable across providers.

Additionally, the system was executed on GitHub-hosted runners included in the free tier of GitHub. Therefore, the performance metrics reflect the behavior of GitHub's cloud-hosted CI environment. While this allows for rapid prototyping and testing, it limits the generalizability of absolute execution times and costs.

The evaluation is based on the QuixBugs benchmark, which includes 40 single-line bugs, each in a separate file. This dataset does not fully represent real-world software development scenarios, as it only covers a narrow set of small algorithmic bugs. Moreover, we assume that passing the provided test suite indicates a correct fix. While QuixBugs offers relatively thorough tests for its scale, they do not guarantee semantic correctness or full behavioral equivalence with the ground truth. Therefore, a generated fix may pass all tests while leaving the program semantically incorrect.

Another important consideration is that QuixBugs is an open-source benchmark published prior to the release of the evaluated \acp{LLM}. It is possible that the dataset or its individual bugs may have appeared in the training data of some models. While this cannot be verified, it represents a further threat to validity and may inflate model performance.

To partially offset these limitations, we evaluated twelve diverse \acp{LLM}. These models vary in capability, architecture, and cost, which provides insight into how different configurations might generalize to larger datasets or other CI environments. However, to draw broader conclusions about real-world effectiveness, further evaluation on more complex benchmarks such as Defects4J or SWE-Bench is necessary.

The threats outlined above limit the scope of our conclusions. Additional testing on other programming languages, codebases, and CI platforms is required to fully validate the approach at production scale. Nevertheless, the results demonstrate that an LLM-based automated bug fixing pipeline can be successfully integrated into a CI workflow and can achieve non-trivial repair rates with minimal time investment and relatively low cost.

\section{Potentials}

Section \ref{chapter:implementation} addresses RQ1 by demonstrating that LLM-based Automated Bug Fixing can effectively be integrated into Continuous Integration workflows through the use of GitHub Actions and containerized APR logic. While the prototype was evaluated in the ``quixbugs-apr'' and ``bugfix-ci'' repositories, the underlying approach is designed for portability and can be extended to any Python project with minimal effort. The use of a YAML configuration file enables rapid adaptation of the pipeline to new repositories and environments. This modularity allows for ongoing adjustments and improvements to the bug-fixing system without requiring changes to the surrounding codebase, making the solution both flexible and maintainable.

By showcasing the resulting workflow in Section \ref{section:showcase}, we demonstrated that, once integrated, producing an automated bug fix requires only a single user action. For example, a developer can simply create and label an issue\footnote{wit the according label}, or schedule a dispatch within the GitHub repository. This highlights the potential of the approach to fully automate the bug-fixing process, eliminating the need for direct developer intervention in the repair workflow. Issue creation and ticketing for features, adjustments, or bugs are fundamental components of agile development. By integrating with this workflow, our system allows automated bug fixes to be triggered directly from these standard agile practices.

The results further indicate that this integration can actively support developers throughout the agile lifecycle by automating the design, development, and testing stages for bug fixes. Developers are left primarily with specifying requirements in the form of issues and reviewing the generated fixes, thus reducing their manual workload. Notably, the system is able to process and resolve issues even when descriptions are minimal, as shown in our evaluation, which relied on issues with little detailed information. This allows developers to focus on more complex and creative tasks, while routine bug fixes are handled automatically. These findings are consistent with the work of Hou et al.~\cite{houLargeLanguageModels2024}, who states that \acp{LLM} can accelerate bug fixing and enhance both software reliability and maintainability.

For RQ2, the evaluation results in Section \ref{section:evaluation-results} demonstrate that the prototype can achieve a repair success rate of up to 100\% on the QuixBugs benchmark, with the best-performing model (o4-mini) successfully repairing all 40 test cases in the few-shot configuration. This highlights the effectiveness of the proposed approach for fixing single-file bugs within a CI environment. However, a deeper look into the collected data reveals that the repair success rate varies across different \acp{LLM} and configurations, indicating a strong dependence on selected \ac{LLM}.

In the zero-shot configuration, where each issue receives only a single repair attempt, several models already achieve high repair success rates. For example, gemini-2.5-pro and gpt-4.1-mini each achieve a success rate of 95\% (38 out of 40 issues), while o4-mini and gpt-4.1 reach 90\%. Even more lightweight models such as gemini-2.0-flash-lite and gemini-2.5-flash-lite-preview-06-17 achieve success rates of 82.5\% and 90\%, respectively. These results indicate that smaller, more cost-efficient models can still deliver strong performance in zero-shot settings, although the highest success rates are achieved by more capable models.

The main advantage of smaller models lies in their cost-effectiveness and rapid execution. For instance, gemini-2.5-flash-lite-preview-06-17 achieves a repair success rate of 90\% in the zero-shot setting, with an average cost per issue of just \$0.0002 and an average execution time of 5.08 seconds. Similarly, gemini-2.0-flash-lite repairs 82.5\% of issues at an extremely low cost of \$0.00015 per issue and an average time of 6.07 seconds. In comparison, larger models such as gemini-2.5-pro reach a 95\% repair rate but at higher costs (\$0.071 per issue) and longer execution times (70.09 seconds per issue). These findings indicate that smaller models can be highly effective for automated bug fixing in CI environments, enabling frequent and rapid submissions of fixes at minimal cost.

Allowing the \ac{LLM} to make multiple repair attempts per issue, using internal feedback loops, significantly improves repair success rates across all models. For example, with up to three attempts (few-shot), the o4-mini model improves from 90\% to 100\% repair success, and gpt-4.1-mini increases from 95\% to 97.5\%. Smaller models such as gpt-4.1-nano show an even larger improvement, rising from 70\% in the single-attempt setting to 90\% with retries. Importantly, these increases in success rate are achieved with a linear rise in cost and execution time. As a result, even lightweight models can reach performance levels similar to larger models while maintaining the advantages of lower cost and faster execution. These findings align with the observations of Brown et al.~\cite{brownLanguageModelsAre2020}, who demonstrated the effectiveness of few-shot learning for large language models.

The measured repair times across all models range from as low as 5 seconds per issue (e.g., gemini-2.5-flash-lite-preview-06-17) up to about 70 seconds per issue for the largest model tested (gemini-2.5-pro). Most models complete repairs well within typical CI job expectations\footnote{Recent research suggests that 10 minutes is the most acceptable build duration \cite{hiltonTradeoffsContinuousIntegration2017}.}, meaning the approach does not impose significant waiting times for developers. In terms of cost, repair attempts vary from as little as \$0.0001 per issue for lightweight models to around \$0.07 per issue for the largest models. These results confirm that automated bug fixing with \acp{LLM} is both time-efficient and cost-effective, making it practical for integration into continuous integration environments without introducing workflow bottlenecks or excessive expenses.

Overall, the combination of high repair success rates, low execution times, and minimal costs demonstrates that this approach is both feasible and practical for automating bug fixing in CI environments. Because each repair task is isolated and stateless, multiple issues can be processed concurrently, further increasing throughput and minimizing delays for development.

By leveraging an agentless architecture together with the interactivity of the GitHub platform, this system achieves repair success rates on the QuixBugs benchmark that are comparable to those reported in recent APR research~\cite{huCanGPTO1Kill2024, }. In addition, the direct integration into the CI workflow provides transparent metrics for both cost and performance, offering practical advantages over many traditional APR approaches.

As highlighted in the results, system effectiveness and efficiency is highly dependent on the underlying language model used. However, the modular design of the approach allows for straightforward integration of improved or newly released models from providers such as OpenAI, Google, or Anthropic. This ensures that the system can benefit from advances in \ac{LLM} technology without requiring major changes to the overall pipeline. Its portability and modularity allow for a wide range of enhancements, such as integrating new \acp{LLM}, experimenting with different prompting strategies or incorporating richer feedback mechanisms, as described in in Section~\ref{section:roadmap}.

\section{Limitations}
%TODO add that githuba action scheduleing is very unrealibale 
Ultimately, there are also limitations faced by the prototype and the overall approach taken.

As mentioned in section \ref{section:validity}, the system is constrained to addressing small issues only. Even with small issues, the timing and availability of the system are highly dependent on external factors such as the \ac{LLM} providers' APIs and the GitHub Actions runners. This presents a limitation in terms of reliability and performance in a real software development cycle.

Regarding integration with GitHub, the system faces additional limitations imposed by the GitHub Actions environment. Runs cannot be skipped, and the filtering logic for events using external configuration data is very limited. As a result, the workflow must execute on every issue labeling event to perform filtering in the first job. This causes the GitHub Actions tab to fill with runs that are marked as successful but do not process any bugs, resulting in a cluttered run history and potential confusion for users. This could be improved by migrating the APR core to a GitHub App that listens to webhook events and only triggers the workflow for relevant issues.

Additionally, security and privacy concerns arise from the fact that program repair relies on \acp{LLM}. Since the issue title and description are added to the prompt used for repair, malicious instructions could be inserted into an issue. Therefore, untrusted project contributors should not be allowed to create or edit issues. Furthermore, code submitted as a pull request to fix a bug must be carefully verified and reviewed before merging.

Lastly, \ac{LLM} providers like Google, OpenAI, and Anthropic are not fully transparent about their data and storage policies. This may raise concerns about the privacy of code and issues processed by the system, especially for private or sensitive repositories. The prototype was not tested using open source models, but it is designed to be modular and extendable for use with open source models. Nevertheless, copyright and licensing issues may arise when code is generated by \acp{LLM} trained on copyrighted data \cite{sauvolaFutureSoftwareDevelopment2024, houLargeLanguageModels2024}.

\section{Lessons Learned}
The development and evaluation of this prototype provided several valuable insights, both technical and practical:

\begin{itemize}
    \item \textbf{Effectiveness of \acp{LLM}:} Large Language Models can automate bug fixing in CI environments, but outcomes are highly dependent on the model's capabilities, context and prompting techniques.
    \item \textbf{Provider and API Reliability:} The APIs of \ac{LLM} providers are fully reliable, with some, such as Googles Gemini-2.5-pro, experiencing instabilities or high latency. Developing retry logic and proper error handling is essential for using external APIs.
    \item \textbf{Rapid Evolution of the Field:} The landscape of \ac{AI} is evolving quickly, with new models and approaches released on a daily basis. Following research and designing modular software are important to stay up to date in this field.
    \item \textbf{Benchmarking Challenges:} Benchmark datasets like QuixBugs provide a starting point for evaluation, but real-world applications will require broader benchmarks like SWE Bench with diverse scenarios to fully assess effectiveness.
    \item \textbf{Human Oversight Remains Important:} While automation can handle routine bugs, human review remains important, particularly for accessing fixes for edge cases, hallucinations and insecurities.
\end{itemize}

Overall, the implementation revealed both the potential and the current limitations of integrating LLM-based program repair into CI and provided valuable insights in the field of \ac{APR}.


\section{Roadmap for Extensions} \label{section:roadmap}
The modular and extensible design of this prototype allows for future improvements, both in research and practical application. Many of the ideas presented here have emerged during implementation and while exploring the latest developments in \ac{APR} research discussed throughout this thesis. Due to time constraints, these directions were not realized, but they represent valuable opportunities for further work.

\begin{itemize}
    \item \textbf{Deeper Data Analysis:} The system already collects logs and metrics, which could be analyzed in greater detail to identify failures and improve localization and repair.
    \item \textbf{Optimization of Cost, Time, and Success Rates:} More extensive experimentation with models, context engineering and prompting strategies could help to further optimize costs and execution times while maximizing repair success rates.
    \item \textbf{Adaptive Model Selection:} Implementing a dynamic approach where lightweight models handle most repairs but challenging cases fall back to more powerful models could improve effectiveness and efficiency.
    \item \textbf{Broader and Richer Benchmarks:} Extending evaluation beyond QuixBugs to include larger and more diverse benchmarks, such as Defects4J or SWE-bench, would provide a stronger basis for generalization and reveal strengths and limitations in real-world settings.
    \item \textbf{Improved Platform Integration:} Integrating the system as a GitHub App that leverages webhook events, or using service accounts for better access control, could simplify development and improve user experience.
    \item \textbf{Experimentation with Agent Architectures:} Testing and comparing multi-agent or interactive agent approaches could uncover additional gains in repair effectiveness.
    \item \textbf{Human Factors and Developer Experience:} Future work could measure developer trust, satisfaction, and acceptance of automated fixes in real teams, supporting adoption in production workflows.
\end{itemize}

These directions illustrate just a few of the many ways the presented approach can serve as a foundation for continued innovation in \acf{APR}.
