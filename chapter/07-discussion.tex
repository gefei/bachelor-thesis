In this section, we will discuss the results of the evaluation of our prototype and put it into context to answer the research questions. First evaluating the validity of our findings, the potential of our approach, its limitations, and summarize the lessons learned. Finally, we will outline a roadmap for future extensions of our work.
% TODO add comparisons to other papers results and approaches

\section{Validity} \label{section:validity}
The results from the evaluation are very promising but still face some limitations to the validity of the results.

%TODO cite + add that tokens and costs are estiamted and not 100% exact
Because the repair process is based on LLMs, which are non-deterministic by design, executions can vary in their results. Furthermore speed and availability of the tested LLMs is highly dependant on the providers APIs which can account for varying execution times during high traffic times. In addition, the system was executed on GitHub provided GitHub Actions runners included in the GitHub free Limits. Therefore the performance metrics reflect GitHubs cloud-hosted CI environments. While allowing quick iterations and setup this limits the feasibility of absolute, execution times and costs.

The bases of evaluation is the QuixBugs benchmark which consists of 40 single line issues each in a separate file. This dataset is not fully representative of real-world software development, as it only covers a small niche from the complexity and variety of bugs that can arise in larger codebases.
In addition we assume that the tests show reliable correctness of a tested program. Although QuixBugs provides extensive tests for the size of the programs, these do not guarantee full behavioral equivalence with the ground truth. Consequently, a generated patch may pass the tests while being semantically incorrect.

We partially offset these limitations by evaluating against twelve diverse LLMs that are likely to translate to larger datasets and other CI platforms. But testing on larger benchmarks like Defects4J and SWE-Bench are required before drawing conclusions about real-world effectiveness.

The results demonstrate that an LLM based automated bug fixing pipeline can be integrated into a CI workflow and achieve non-trivial repair rates at with minimal time effort and low cost.
Nevertheless, the threats outlined above delimit the scope of that claim. More benchmarks and programming languages are necessary to fully validate the approach in production scale settings.

--add that knowledge cutoff is after the benchmark was publishes since the benchmark is open source it might have been in the training dataset of the models

\section{Potentials}

\ref{section:showcase} shows how the self developed approach allows for LLM based automated bug fixing in a CI workflow. 

hand of fixing small bugs:
- can take over small tasks in encapsulated environment without intervention
- the workflow shows that bugs can be fixed by just adding a label to an issue

- small models can solve more problems with retrying with feedback
- smaller models are very fast and cost effective
- no description is needed to solve small issues
- this concept is applicable to other python repositories
- configuration makes it adjustable to different repositories and environments
- similar results to other approaches -> is feasible

the results show promising reapir effectiveness of X\% up the cheapest model X

- combines agentless and a bit of interactive but interactivity is limited due to timings but can resemble real world remote environment

-with small models and attempt loop makes small models pass the whole benchmark?

agent architectures produce good results epically paired with containerized environments. \cite{puvvadiCodingAgentsComprehensive2025}

- the framework is model agnostic and as LLMs get better the

\ref{houLargeLanguageModels2024}
- accelerate bug fixing
- lets developers focus on more complex tasks
- therefor enhance software reliability and maintainability


this setup is portable, modular and extendable so it can be adaped and further tests can be made more in \ref{section:roadmap}

\section{Limitations}
Ultimately, there are also limitations faced by the prototype and the approach in general.

As mentioned in the validity section \ref{section:validity}, the system is contained to addressing small issues only. Even with small issues the timings and availability of the system is highly dependant on external dependencies like the LLM providers APIs and the provided GitHub Actions runners. Which makes this a limitation in terms of reliability and performance in a real software development cycle.

In terms of the integration with GitHub is that the system faces more limitations that come from the Github Actions environment. Runs can not be skipped and filtering logic of events with external configuration data is very limited. This results in the workflow having to execute on every issue labeling event to filter in the first job. This fills the GitHub Actions tab with runs that have success as status but do not have any bugs to fix, resulting in cluttered the run history which may lead to confusion for users. This could be solved by migrating the APR core to a GitHub App which listens to webhook events and only triggers the workflow for relevant issues.

Additionally security and privacy concerns arise from the fact that the program repair is based on LLMs. Since issue title and description get added to the prompt that is used for repair, malicious instructions could be put into an issue. Therefore non trusted project contributors should not be able to create or edit issues. Furthermore code submitted for fixing the bug as a pull request needs to be verified and reviewed carefully before merging.

Large LLM providers like Google, OpenAI and Anthropic are not fully transparent about their data and storage policies. This may raise concerns about the privacy of the code and issues processed by the system, especially for private or sensitive repositories. The prototype was not tested using open source models, but is by design modular and  extendable for the use of open source model. Nevertheless, copyright and licensing issues may arise when code is generated by LLMs that are based on copyrighted training data. \cite{sauvolaFutureSoftwareDevelopment2024, houLargeLanguageModels2024}


\section{Lessons Learned}
The development and evaluation of the prototype was an interesting and insightful experience. The following lessons were learned during the process:
- LLMs can be used to automate bug fixing in a CI environment, but the results are highly dependent on the quality of the LLM and the training data.
- The field of LLMs is rapidly evolving, and new models are released frequently. This makes it difficult to keep up with the latest developments.


\section{Roadmap for Extensions} \label{section:roadmap}
- richer benchmarks
- Service Accounts for better and more transparent integration
- try out complex agent architectures and compare metrics and results
- try out more complex bug fixing tasks - SWE bench
- concurrency and parallelization of tasks
- app which replies on webhook events
- measure developer trust and satisfaction