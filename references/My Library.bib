@misc{29HowWe,
  title = {(29) {{How We Build Effective Agents}}: {{Barry Zhang}}, {{Anthropic}} - {{YouTube}}},
  urldate = {2025-04-13},
  howpublished = {https://www.youtube.com/watch?v=D7\_ipDqhtwk},
  file = {/Users/justingebert/Zotero/storage/DAKFHRBD/watch.html}
}

@misc{50SelfCoding,
  title = {(50) {{Self Coding Agents}} ---~{{Colin Flaherty}}, {{Augment Code}} - {{YouTube}}},
  urldate = {2025-04-22},
  howpublished = {https://www.youtube.com/watch?v=Iw\_3cRf3lnM\&ab\_channel=AIEngineer},
  file = {/Users/justingebert/Zotero/storage/FWMJG6SA/watch.html}
}

@misc{AutomatedBugFixing,
  title = {Automated {{Bug Fixing}}: {{From Templates}} to {{AI Agents}}},
  shorttitle = {Automated {{Bug Fixing}}},
  journal = {dzone.com},
  urldate = {2025-03-10},
  abstract = {Automated bug fixing has evolved from simple template-based approaches to sophisticated AI systems powered by LLMs, agents, agentless, and RAG paradigms.},
  howpublished = {https://dzone.com/articles/automated-bug-fixing-from-templates-to-ai-agents},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/6FVM3CC7/automated-bug-fixing-from-templates-to-ai-agents.html}
}

@article{baderGetafixLearningFix2019,
  title = {Getafix: Learning to Fix Bugs Automatically},
  shorttitle = {Getafix},
  author = {Bader, Johannes and Scott, Andrew and Pradel, Michael and Chandra, Satish},
  year = {2019},
  month = oct,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {OOPSLA},
  pages = {1--27},
  issn = {2475-1421},
  doi = {10.1145/3360585},
  urldate = {2025-03-06},
  abstract = {Static analyzers help find bugs early by warning about recurring bug categories. While fixing these bugs still remains a mostly manual task in practice, we observe that fixes for a specific bug category often are repetitive. This paper addresses the problem of automatically fixing instances of common bugs by learning from past fixes. We present Getafix, an approach that produces human-like fixes while being fast enough to suggest fixes in time proportional to the amount of time needed to obtain static analysis results in the first place. Getafix is based on a novel hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns. Instead of an expensive exploration of a potentially large space of candidate fixes, Getafix uses a simple yet effective ranking technique that uses the context of a code change to select the most appropriate fix for a given bug. Our evaluation applies Getafix to 1,268 bug fixes for six bug categories reported by popular static analyzers for Java, including null dereferences, incorrect API calls, and misuses of particular language constructs. The approach predicts exactly the human-written fix as the top-most suggestion between 12\% and 91\% of the time, depending on the bug category. The top-5 suggestions contain fixes for 526 of the 1,268 bugs. Moreover, we report on deploying the approach within Facebook, where it contributes to the reliability of software used by billions of people. To the best of our knowledge, Getafix is the first industrially-deployed automated bug-fixing tool that learns fix patterns from past, human-written fixes to produce human-like fixes. CCS Concepts: {$\bullet$} Software and its engineering {$\rightarrow$} Software testing and debugging.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5LZ3GY8Y/Bader et al. - 2019 - Getafix learning to fix bugs automatically.pdf}
}

@misc{bytebytegoAutomatedBugFixing2023,
  title = {Automated {{Bug Fixing}} at {{Facebook Scale}}},
  author = {ByteByteGo},
  year = {2023},
  month = apr,
  urldate = {2025-03-06},
  abstract = {ðŸ“ŒSave the Date!},
  howpublished = {https://blog.bytebytego.com/p/automated-bug-fixing-at-facebook},
  langid = {english}
}

@misc{changBridgingBugLocalization2025,
  title = {Bridging {{Bug Localization}} and {{Issue Fixing}}: {{A Hierarchical Localization Framework Leveraging Large Language Models}}},
  shorttitle = {Bridging {{Bug Localization}} and {{Issue Fixing}}},
  author = {Chang, Jianming and Zhou, Xin and Wang, Lulu and Lo, David and Li, Bixin},
  year = {2025},
  month = feb,
  number = {arXiv:2502.15292},
  eprint = {2502.15292},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.15292},
  urldate = {2025-03-24},
  abstract = {Automated issue fixing is a critical task in software debugging and has recently garnered significant attention from academia and industry. However, existing fixing techniques predominantly focus on the repair phase, often overlooking the importance of improving the preceding bug localization phase. As a foundational step in issue fixing, bug localization plays a pivotal role in determining the overall effectiveness of the entire process. To enhance the precision of issue fixing by accurately identifying bug locations in large-scale projects, this paper presents BugCerberus, the first hierarchical bug localization framework powered by three customized large language models. First, BugCerberus analyzes intermediate representations of bug-related programs at file, function, and statement levels and extracts bug-related contextual information from the representations. Second, BugCerberus designs three customized LLMs at each level using bug reports and contexts to learn the patterns of bugs. Finally, BugCerberus hierarchically searches for bug-related code elements through well-tuned models to localize bugs at three levels. With BugCerberus, we further investigate the impact of bug localization on the issue fixing. We evaluate BugCerberus on the widely-used benchmark SWE-bench-lite. The experimental results demonstrate that BugCerberus outperforms all baselines. Specifically, at the fine-grained statement level, BugCerberus surpasses the state-of-the-art in Top-N (N=1, 3, 5, 10) by 16.5\%, 5.4\%, 10.2\%, and 23.1\%, respectively. Moreover, in the issue fixing experiments, BugCerberus improves the fix rate of the existing issue fixing approach Agentless by 17.4\% compared to the best baseline, highlighting the significant impact of enhanced bug localization on automated issue fixing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/JYN6JPF5/Chang et al. - 2025 - Bridging Bug Localization and Issue Fixing A Hierarchical Localization Framework Leveraging Large L.pdf;/Users/justingebert/Zotero/storage/QX9UK4WN/2502.html}
}

@misc{chenUnveilingPitfallsUnderstanding2025,
  title = {Unveiling {{Pitfalls}}: {{Understanding Why AI-driven Code Agents Fail}} at {{GitHub Issue Resolution}}},
  shorttitle = {Unveiling {{Pitfalls}}},
  author = {Chen, Zhi and Ma, Wei and Jiang, Lingxiao},
  year = {2025},
  month = mar,
  number = {arXiv:2503.12374},
  eprint = {2503.12374},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.12374},
  urldate = {2025-03-24},
  abstract = {AI-driven software development has rapidly advanced with the emergence of software development agents that leverage large language models (LLMs) to tackle complex, repository-level software engineering tasks. These agents go beyond just generation of final code; they engage in multi-step reasoning, utilize various tools for code modification and debugging, and interact with execution environments to diagnose and iteratively resolve issues. However, most existing evaluations focus primarily on static analyses of final code outputs, yielding limited insights into the agents' dynamic problem-solving processes. To fill this gap, we conduct an in-depth empirical study on 3,977 solving-phase trajectories and 3,931 testing-phase logs from 8 top-ranked agents evaluated on 500 GitHub issues in the SWE-Bench benchmark. Our exploratory analysis shows that Python execution errors during the issue resolution phase correlate with lower resolution rates and increased reasoning overheads. We have identified the most prevalent errors -- such as ModuleNotFoundError and TypeError -- and highlighted particularly challenging errors like OSError and database-related issues (e.g., IntegrityError) that demand significantly more debugging effort. Furthermore, we have discovered 3 bugs in the SWE-Bench platform that affect benchmark fairness and accuracy; these issues have been reported to and confirmed by the maintainers. To promote transparency and foster future research, we publicly share our datasets and analysis scripts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/WDWRSFMR/Chen et al. - 2025 - Unveiling Pitfalls Understanding Why AI-driven Code Agents Fail at GitHub Issue Resolution.pdf;/Users/justingebert/Zotero/storage/WPC2PKQA/2503.html}
}

@inproceedings{gyimesiBugsJSBenchmarkJavaScript2019,
  title = {{{BugsJS}}: A {{Benchmark}} of {{JavaScript Bugs}}},
  shorttitle = {{{BugsJS}}},
  booktitle = {2019 12th {{IEEE Conference}} on {{Software Testing}}, {{Validation}} and {{Verification}} ({{ICST}})},
  author = {Gyimesi, P{\'e}ter and Vancsics, B{\'e}la and Stocco, Andrea and Mazinanian, Davood and Besz{\'e}des, {\'A}rp{\'a}d and Ferenc, Rudolf and Mesbah, Ali},
  year = {2019},
  month = apr,
  pages = {90--101},
  issn = {2159-4848},
  doi = {10.1109/ICST.2019.00019},
  urldate = {2025-03-27},
  abstract = {JavaScript is a popular programming language that is also error-prone due to its asynchronous, dynamic, and loosely-typed nature. In recent years, numerous techniques have been proposed for analyzing and testing JavaScript applications. However, our survey of the literature in this area revealed that the proposed techniques are often evaluated on different datasets of programs and bugs. The lack of a commonly used benchmark limits the ability to perform fair and unbiased comparisons for assessing the efficacy of new techniques. To fill this gap, we propose BugsJS, a benchmark of 453 real, manually validated JavaScript bugs from 10 popular JavaScript server-side programs, comprising 444k LOC in total. Each bug is accompanied by its bug report, the test cases that detect it, as well as the patch that fixes it. BugsJS features a rich interface for accessing the faulty and fixed versions of the programs and executing the corresponding test cases, which facilitates conducting highly-reproducible empirical studies and comparisons of JavaScript analysis and testing tools.},
  keywords = {benchmark,Benchmark testing,bug database,BugsJS,Computer bugs,Concurrent computing,JavaScript,literature survey,Maintenance engineering,real bugs,reproducibility,Software,Test pattern generators},
  file = {/Users/justingebert/Zotero/storage/6MVXLEHH/Gyimesi et al. - 2019 - BugsJS a Benchmark of JavaScript Bugs.pdf;/Users/justingebert/Zotero/storage/KPCI53W2/8730197.html}
}

@article{hossainDeepDiveLarge2024,
  title = {A {{Deep Dive}} into {{Large Language Models}} for {{Automated Bug Localization}} and {{Repair}}},
  author = {Hossain, Soneya Binta and Jiang, Nan and Zhou, Qiang and Li, Xiaopeng and Chiang, Wen-Hao and Lyu, Yingjun and Nguyen, Hoan and Tripp, Omer},
  year = {2024},
  month = jul,
  journal = {Proceedings of the ACM on Software Engineering},
  volume = {1},
  number = {FSE},
  pages = {1471--1493},
  issn = {2994-970X},
  doi = {10.1145/3660773},
  urldate = {2025-03-13},
  abstract = {Large language models (LLMs) have shown impressive effectiveness in various software engineering tasks,                                                               including automated program repair (APR). In this study, we take a deep dive into automated bug localization                                                               and repair utilizing LLMs. In contrast to many deep learning-based APR methods that assume known bug                                                               locations, rely on line-level localization tools, or address bug prediction and fixing in one step, our approach                                                               uniquely employs LLMs to predict bug location at the token level and subsequently utilizes them for bug                                                               fixing. This methodological separation of bug localization and fixing using different LLMs enables effective                                                               integration of diverse contextual information and improved incorporation of inductive biases. We introduce                                                               Toggle: Token-Granulated Bug Localization and Repair, a comprehensive program repair framework                                                               that integrates a bug localization model, an adjustment model to address tokenizer inconsistencies, and a                                                               bug-fixing model. Toggle takes a buggy function as input and generates a complete corrected function. We                                                               investigate various styles of prompting to the bug fixing model to identify the most effective prompts that                                                               better utilize the inductive bias and significantly outperform others. Toggle achieves the new state-of-the-art                                                               (SOTA) performance on the CodeXGLUE code refinement benchmark, and exhibits better and comparable                                                               performance on several other widely-used APR datasets, including Defects4J. In the Defects4J benchmark, our                                                               approach consistently ranks above other methods, achieving superior results in the Top-10, Top-30, Top-50,                                                               and Top-100 metrics. Besides examining Toggle's generalizability to unseen data, evaluating the effectiveness                                                               of various prompts, we also investigate the impact of additional contextual information such as buggy lines                                                               and code comments on bug localization, and explore the importance of the adjustment model. Our extensive                                                               experiments offer valuable insights and answers to critical research questions.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/3T4HUBN7/Hossain et al. - 2024 - A Deep Dive into Large Language Models for Automated Bug Localization and Repair.pdf}
}

@misc{houLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Software Engineering}}: {{A Systematic Literature Review}}},
  shorttitle = {Large {{Language Models}} for {{Software Engineering}}},
  author = {Hou, Xinyi and Zhao, Yanjie and Liu, Yue and Yang, Zhou and Wang, Kailong and Li, Li and Luo, Xiapu and Lo, David and Grundy, John and Wang, Haoyu},
  year = {2024},
  month = apr,
  number = {arXiv:2308.10620},
  eprint = {2308.10620},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.10620},
  urldate = {2025-03-06},
  abstract = {Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE\_SLR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/9NMUPS2R/Hou et al. - 2024 - Large Language Models for Software Engineering A Systematic Literature Review.pdf;/Users/justingebert/Zotero/storage/48ARVL3M/2308.html}
}

@misc{huCanGPTO1Kill2024,
  title = {Can {{GPT-O1 Kill All Bugs}}? {{An Evaluation}} of {{GPT-Family LLMs}} on {{QuixBugs}}},
  shorttitle = {Can {{GPT-O1 Kill All Bugs}}?},
  author = {Hu, Haichuan and Shang, Ye and Xu, Guolin and He, Congqing and Zhang, Quanjun},
  year = {2024},
  month = dec,
  number = {arXiv:2409.10033},
  eprint = {2409.10033},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.10033},
  urldate = {2025-04-15},
  abstract = {LLMs have long demonstrated remarkable effectiveness in automatic program repair (APR), with OpenAI's ChatGPT being one of the most widely used models in this domain. Through continuous iterations and upgrades of GPT-family models, their performance in fixing bugs has already reached state-of-the-art levels. However, there are few works comparing the effectiveness and variations of different versions of GPT-family models on APR. In this work, inspired by the recent public release of the GPT-o1 models, we conduct the first study to compare the effectiveness of different versions of the GPT-family models in APR. We evaluate the performance of the latest version of the GPT-family models (i.e., O1-preview and O1-mini), GPT-4o, and the historical version of ChatGPT on APR. We conduct an empirical study of the four GPT-family models against other LLMs and APR techniques on the QuixBugs benchmark from multiple evaluation perspectives, including repair success rate, repair cost, response length, and behavior patterns. The results demonstrate that O1's repair capability exceeds that of prior GPT-family models, successfully fixing all 40 bugs in the benchmark. Our work can serve as a foundation for further in-depth exploration of the applications of GPT-family models in APR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/BZQGPMP2/Hu et al. - 2024 - Can GPT-O1 Kill All Bugs An Evaluation of GPT-Family LLMs on QuixBugs.pdf;/Users/justingebert/Zotero/storage/VDXHEKBG/2409.html}
}

@misc{jimenezSWEbenchCanLanguage2024,
  title = {{{SWE-bench}}: {{Can Language Models Resolve Real-World GitHub Issues}}?},
  shorttitle = {{{SWE-bench}}},
  author = {Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  year = {2024},
  month = nov,
  number = {arXiv:2310.06770},
  eprint = {2310.06770},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06770},
  urldate = {2025-03-06},
  abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of \$2,294\$ software engineering problems drawn from real GitHub issues and corresponding pull requests across \$12\$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere \$1.96\$\% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/CS85ZE5R/Jimenez et al. - 2024 - SWE-bench Can Language Models Resolve Real-World GitHub Issues.pdf;/Users/justingebert/Zotero/storage/LTFAECDB/2310.html}
}

@article{kodamasimhamkrishnaExploringSynergyGenerative2024a,
  title = {Exploring the Synergy between Generative {{AI}} and Software Engineering: {{Automating}} Code Optimization and Bug Fixing},
  shorttitle = {Exploring the Synergy between Generative {{AI}} and Software Engineering},
  author = {{Kodamasimham Krishna} and {Pranav Murthy} and {Saumya Sarangi}},
  year = {2024},
  month = oct,
  journal = {World Journal of Advanced Engineering Technology and Sciences},
  volume = {13},
  number = {1},
  pages = {682--691},
  issn = {25828266},
  doi = {10.30574/wjaets.2024.13.1.0464},
  urldate = {2025-05-18},
  abstract = {As applied to software engineering, generative AI is quickly transitioning from a zero-sum industry game changer into the primary automation tool for code optimization, bug identification, and problem-solving. This technology takes advantage of artificial intelligence algorithms within machine learning models to analyze and write code, resulting in improved quality and speed of an application development process. The generative AI replenishes productivity in development work and enhances centralization between development work teams through code handling and intelligent suggestions for essential codes. However, the integration of AI in software engineering poses the following problems and ethical questions: the question of accuracy, bias, and data. This paper will review the existing knowledge on generative AI in software engineering regarding its current use, future evolutions and advancements, issues and limitations, and ethical factors in using this technology. This paper considers these aspects to give a global outlook on how generative AI will transform software development in the future and how responsible AI should be employed.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/X7N32X5P/Kodamasimham Krishna et al. - 2024 - Exploring the synergy between generative AI and software engineering Automating code optimization a.pdf}
}

@misc{leeUnifiedDebuggingApproach2024,
  title = {A {{Unified Debugging Approach}} via {{LLM-Based Multi-Agent Synergy}}},
  author = {Lee, Cheryl and Xia, Chunqiu Steven and Yang, Longji and Huang, Jen-tse and Zhu, Zhouruixin and Zhang, Lingming and Lyu, Michael R.},
  year = {2024},
  month = oct,
  number = {arXiv:2404.17153},
  eprint = {2404.17153},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.17153},
  urldate = {2025-03-06},
  abstract = {Software debugging is a time-consuming endeavor involving a series of steps, such as fault localization and patch generation, each requiring thorough analysis and a deep understanding of the underlying logic. While large language models (LLMs) demonstrate promising potential in coding tasks, their performance in debugging remains limited. Current LLM-based methods often focus on isolated steps and struggle with complex bugs. In this paper, we propose the first end-to-end framework, FixAgent, for unified debugging through multi-agent synergy. It mimics the entire cognitive processes of developers, with each agent specialized as a particular component of this process rather than mirroring the actions of an independent expert as in previous multi-agent systems. Agents are coordinated through a three-level design, following a cognitive model of debugging, allowing adaptive handling of bugs with varying complexities. Experiments on extensive benchmarks demonstrate that FixAgent significantly outperforms state-of-the-art repair methods, fixing 1.25\${\textbackslash}times\$ to 2.56\${\textbackslash}times\$ bugs on the repo-level benchmark, Defects4J. This performance is achieved without requiring ground-truth root-cause code statements, unlike the baselines. Our source code is available on https://github.com/AcceptePapier/UniDebugger.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/QMBPI5RQ/Lee et al. - 2024 - A Unified Debugging Approach via LLM-Based Multi-Agent Synergy.pdf;/Users/justingebert/Zotero/storage/DEHKRQZS/2404.html}
}

@inproceedings{linQuixBugsMultilingualProgram2017,
  title = {{{QuixBugs}}: A Multi-Lingual Program Repair Benchmark Set Based on the Quixey Challenge},
  shorttitle = {{{QuixBugs}}},
  booktitle = {Proceedings {{Companion}} of the 2017 {{ACM SIGPLAN International Conference}} on {{Systems}}, {{Programming}}, {{Languages}}, and {{Applications}}: {{Software}} for {{Humanity}}},
  author = {Lin, Derrick and Koppel, James and Chen, Angela and {Solar-Lezama}, Armando},
  year = {2017},
  month = oct,
  pages = {55--56},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/3135932.3135941},
  urldate = {2025-04-17},
  isbn = {978-1-4503-5514-8},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/DAPQAHKW/Lin et al. - 2017 - QuixBugs a multi-lingual program repair benchmark set based on the quixey challenge.pdf}
}

@misc{liuMarsCodeAgentAInative2024,
  title = {{{MarsCode Agent}}: {{AI-native Automated Bug Fixing}}},
  shorttitle = {{{MarsCode Agent}}},
  author = {Liu, Yizhou and Gao, Pengfei and Wang, Xinchen and Liu, Jie and Shi, Yexuan and Zhang, Zhao and Peng, Chao},
  year = {2024},
  month = sep,
  number = {arXiv:2409.00899},
  eprint = {2409.00899},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.00899},
  urldate = {2025-03-06},
  abstract = {Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/PUZ87WBA/Liu et al. - 2024 - MarsCode Agent AI-native Automated Bug Fixing.pdf;/Users/justingebert/Zotero/storage/DJDUQX4T/2409.html}
}

@inproceedings{margineanSapFixAutomatedEndtoEnd2019,
  title = {{{SapFix}}: {{Automated End-to-End Repair}} at {{Scale}}},
  shorttitle = {{{SapFix}}},
  booktitle = {2019 {{IEEE}}/{{ACM}} 41st {{International Conference}} on {{Software Engineering}}: {{Software Engineering}} in {{Practice}} ({{ICSE-SEIP}})},
  author = {Marginean, Alexandru and Bader, Johannes and Chandra, Satish and Harman, Mark and Jia, Yue and Mao, Ke and Mols, Alexander and Scott, Andrew},
  year = {2019},
  month = may,
  pages = {269--278},
  publisher = {IEEE},
  address = {Montreal, QC, Canada},
  doi = {10.1109/ICSE-SEIP.2019.00039},
  urldate = {2025-03-06},
  abstract = {We report our experience with SAPFIX: the first deployment of automated end-to-end fault fixing, from test case design through to deployed repairs in production code1. We have used SAPFIX at Facebook to repair 6 production systems, each consisting of tens of millions of lines of code, and which are collectively used by hundreds of millions of people worldwide.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  isbn = {978-1-7281-1760-7},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/TQFLNXJT/Marginean et al. - 2019 - SapFix Automated End-to-End Repair at Scale.pdf}
}

@misc{mengEmpiricalStudyLLMbased2024,
  title = {An {{Empirical Study}} on {{LLM-based Agents}} for {{Automated Bug Fixing}}},
  author = {Meng, Xiangxin and Ma, Zexiong and Gao, Pengfei and Peng, Chao},
  year = {2024},
  month = nov,
  number = {arXiv:2411.10213},
  eprint = {2411.10213},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.10213},
  urldate = {2025-03-06},
  abstract = {Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system's overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/PPCPHLM8/Meng et al. - 2024 - An Empirical Study on LLM-based Agents for Automated Bug Fixing.pdf;/Users/justingebert/Zotero/storage/B83LA7R4/2411.html}
}

@article{merkelTypeScriptApplicationsShow,
  title = {Do {{TypeScript Applications Show Better Software Quality}} than {{JavaScript Applications}}? {{A Repository Mining Study}} on {{GitHub}}},
  author = {Merkel, Manuel},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/7Q3H66GL/Merkel - Do TypeScript Applications Show Better Software Quality than JavaScript Applications A Repository M.pdf}
}

@inproceedings{mohammedAIDrivenContinuousIntegration2024,
  title = {{{AI-Driven Continuous Integration}} and {{Continuous Deployment}} in {{Software Engineering}}},
  booktitle = {2024 2nd {{International Conference}} on {{Disruptive Technologies}} ({{ICDT}})},
  author = {Mohammed, Abdul Sajid and Saddi, Venkata Ramana and Gopal, Santhosh Kumar and Dhanasekaran, S. and Naruka, Mahaveer Singh},
  year = {2024},
  month = mar,
  pages = {531--536},
  doi = {10.1109/ICDT61202.2024.10489475},
  urldate = {2025-04-23},
  abstract = {AI driven Continuous Integration and Continuous Deployment is a new way of managing and continually updating a software project. This process, powered by Artificial Intelligence, automates the entire software delivery and deployment process - from code submission to monitoring and bug fixing. It eliminates manual errors and allows for multiple versions to be tested in parallel, saving time and effort. By increasing agility, it allows organizations to launch new features to production faster than ever. Continuous Integration and Continuous Deployment leverages artificial intelligence in implementation and execution. It automates the process of integration, testing, packaging and deployment. Furthermore, AI is used to detect and fix bugs which can prevent delays and costly production bugs. AI driven Continuous Integration and Continuous Deployment has become an increasingly popular development strategy. It helps reduce the overall cost and accelerate the software's production cycles, making it easier for developers to quickly get their features and services in the hands of the market.},
  keywords = {Adaptive Systems,Artificial intelligence,Artificial Intelligence,Automation,Cloud Computing,Companies,Computer bugs,Machine Learning,Manuals,Production,Software,Systematics},
  file = {/Users/justingebert/Zotero/storage/6XQC62BK/Mohammed et al. - 2024 - AI-Driven Continuous Integration and Continuous Deployment in Software Engineering.pdf}
}

@inproceedings{puvvadiCodingAgentsComprehensive2025,
  title = {Coding {{Agents}}: {{A Comprehensive Survey}} of {{Automated Bug Fixing Systems}} and {{Benchmarks}}},
  shorttitle = {Coding {{Agents}}},
  booktitle = {2025 {{IEEE}} 14th {{International Conference}} on {{Communication Systems}} and {{Network Technologies}} ({{CSNT}})},
  author = {Puvvadi, Meghana and Arava, Sai Kumar and Santoria, Adarsh and Chennupati, Sesha Sai Prasanna and Puvvadi, Harsha Vardhan},
  year = {2025},
  month = mar,
  pages = {680--686},
  issn = {2473-5655},
  doi = {10.1109/CSNT64827.2025.10968728},
  urldate = {2025-04-27},
  abstract = {One of the trickiest problems in software engineering is automating software issue fixes, which calls for a thorough comprehension of contextual relationships, code semantics, and dynamic debugging techniques. The development of automatic program repair (APR) is examined in this survey, which traces a path from early template and constraint-based approaches to more recent developments powered by large language models (LLMs). Three main paradigms are compared here: retrieval-augmented approaches that integrate external knowledge sources, agent-based systems that use multi-agent frameworks, and agentless systems that use simplified repair pipelines. Real-world benchmarks that mimic actual engineering workflows and repository-level difficulties, such as SWE-bench, CODEAGENT-BENCH, and CodeRAG-Bench, are used to assess these cutting-edge technologies. This study demonstrates how agentic, agentless, and retrieval-augmented systems use LLMs to achieve previously unheard-of precision and scalability by following the shift from localized, single-file solutions to solving complicated, multi-file, and repository-wide difficulties. According to our findings, while complex agent architectures have potential, straightforward test-time scaling frequently produces better outcomes, especially when paired with containerized environments that allow for parallel exploration. Additionally, the survey looks at industrial applications, emphasizing effective connections with quality assurance and DevOps procedures. In order to further the development of more resilient and flexible APR frameworks that blend in perfectly with contemporary software engineering practices, we conclude by highlighting important issues in context handling and validation and suggesting future research directions in improved contextual models, human-AI collaboration, and multi-modal debugging systems.},
  keywords = {Agent-Based Models,AI in Software Development,Automated Program Repair,Benchmark testing,Context modeling,Context-Aware Debugging,Debugging,Debugging Benchmarks,Large language models,Large Language Models,Maintenance engineering,Multi-Agent Systems,Scalability,Semantics,Software,Software engineering,Software Engineering Automation,Surveys},
  file = {/Users/justingebert/Zotero/storage/K4FG4JCB/Puvvadi et al. - 2025 - Coding Agents A Comprehensive Survey of Automated Bug Fixing Systems and Benchmarks.pdf}
}

@misc{rondonEvaluatingAgentbasedProgram2025,
  title = {Evaluating {{Agent-based Program Repair}} at {{Google}}},
  author = {Rondon, Pat and Wei, Renyao and Cambronero, Jos{\'e} and Cito, J{\"u}rgen and Sun, Aaron and Sanyam, Siddhant and Tufano, Michele and Chandra, Satish},
  year = {2025},
  month = jan,
  number = {arXiv:2501.07531},
  eprint = {2501.07531},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.07531},
  urldate = {2025-03-24},
  abstract = {Agent-based program repair offers to automatically resolve complex bugs end-to-end by combining the planning, tool use, and code generation abilities of modern LLMs. Recent work has explored the use of agent-based repair approaches on the popular open-source SWE-Bench, a collection of bugs from highly-rated GitHub Python projects. In addition, various agentic approaches such as SWE-Agent have been proposed to solve bugs in this benchmark. This paper explores the viability of using an agentic approach to address bugs in an enterprise context. To investigate this, we curate an evaluation set of 178 bugs drawn from Google's issue tracking system. This dataset spans both human-reported (78) and machine-reported bugs (100). To establish a repair performance baseline on this benchmark, we implement Passerine, an agent similar in spirit to SWE-Agent that can work within Google's development environment. We show that with 20 trajectory samples and Gemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e., plausible) for 73\% of machine-reported and 25.6\% of human-reported bugs in our evaluation set. After manual examination, we found that 43\% of machine-reported bugs and 17.9\% of human-reported bugs have at least one patch that is semantically equivalent to the ground-truth patch. These results establish a baseline on an industrially relevant benchmark, which as we show, contains bugs drawn from a different distribution -- in terms of language diversity, size, and spread of changes, etc. -- compared to those in the popular SWE-Bench dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/FVL86YKC/Rondon et al. - 2025 - Evaluating Agent-based Program Repair at Google.pdf;/Users/justingebert/Zotero/storage/XHNQAEUA/2501.html}
}

@inproceedings{sobaniaAnalysisAutomaticBug2023,
  title = {An {{Analysis}} of the {{Automatic Bug Fixing Performance}} of {{ChatGPT}}},
  booktitle = {2023 {{IEEE}}/{{ACM International Workshop}} on {{Automated Program Repair}} ({{APR}})},
  author = {Sobania, Dominik and Briesch, Martin and Hanna, Carol and Petke, Justyna},
  year = {2023},
  month = may,
  pages = {23--30},
  doi = {10.1109/APR59189.2023.00012},
  urldate = {2025-03-06},
  abstract = {To support software developers in finding and fixing software bugs, several automated program repair techniques have been introduced. Given a test suite, standard methods usually either synthesize a repair, or navigate a search space of software edits to find test-suite passing variants. Recent program repair methods are based on deep learning approaches. One of these novel methods, which is not primarily intended for automated program repair, but is still suitable for it, is ChatGPT. The bug fixing performance of ChatGPT, however, is so far unclear. Therefore, in this paper we evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs, and compare the performance with the results of several other approaches reported in the literature. We find that ChatGPT's bug fixing performance is competitive to the common deep learning approaches CoCoNut and Codex and notably better than the results reported for the standard program repair approaches. In contrast to previous approaches, ChatGPT offers a dialogue system through which further information, e.g., the expected output for a certain input or an observed error message, can be entered. By providing such hints to ChatGPT, its success rate can be further increased, fixing 31 out of 40 bugs, outperforming state-of-the-art.},
  keywords = {Automated program repair,automatic bug fixing,Benchmark testing,Chatbots,ChatGPT,Codex,Computer bugs,Deep learning,language models,Maintenance engineering,Navigation,Source coding},
  file = {/Users/justingebert/Zotero/storage/86DUHN6D/Sobania et al. - 2023 - An Analysis of the Automatic Bug Fixing Performance of ChatGPT.pdf;/Users/justingebert/Zotero/storage/EQ7Z8GHT/10189263.html}
}

@article{tufanoEmpiricalStudyLearning2019,
  title = {An {{Empirical Study}} on {{Learning Bug-Fixing Patches}} in the {{Wild}} via {{Neural Machine Translation}}},
  author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Penta, Massimiliano Di and White, Martin and Poshyvanyk, Denys},
  year = {2019},
  month = oct,
  journal = {ACM Transactions on Software Engineering and Methodology},
  volume = {28},
  number = {4},
  pages = {1--29},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3340544},
  urldate = {2025-04-08},
  abstract = {Millions of open source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation, we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9--50\% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/DG9PG5SN/Tufano et al. - 2019 - An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation.pdf}
}

@article{ugwuezeContinuousIntegrationDeployment2024,
  title = {Continuous {{Integration}} and {{Deployment Strategies}} for {{Streamlined DevOps}} in {{Software Engineering}} and {{Application Delivery}}},
  author = {Ugwueze, Vincent and Chukwunweike, Joseph},
  year = {2024},
  month = jan,
  journal = {International Journal of Computer Applications Technology and Research},
  pages = {1--24},
  doi = {10.7753/IJCATR1401.1001},
  abstract = {In modern software engineering, Continuous Integration (CI) and Continuous Deployment (CD) have emerged as essential practices for improving the efficiency and reliability of software delivery. These practices form the backbone of DevOps, a set of methodologies that bridges the gap between development and operations, fostering collaboration and automating the delivery pipeline. The concept of CI involves the frequent integration of code changes into a shared repository, allowing for early detection of bugs and ensuring that new code aligns with the project's standards. CD extends this by automating the deployment of code changes into production, enabling frequent and reliable releases without manual intervention. This paper explores the strategies and tools that enable seamless integration and deployment in software engineering. It examines the role of version control systems, automated testing, and containerization technologies such as Docker in optimizing CI/CD workflows. The challenges associated with scaling CI/CD pipelines, handling microservices architectures, and maintaining security throughout the deployment process are discussed in detail. Additionally, this paper highlights the importance of monitoring and feedback loops for continuous improvement and the adoption of best practices in DevOps, such as automation, collaboration, and rapid iteration. By embracing CI/CD strategies, organizations can reduce time-to-market, enhance software quality, and increase deployment frequency, ultimately streamlining DevOps processes and accelerating application delivery. This paper provides insights into the transformative impact of CI/CD practices on the software engineering lifecycle, offering practical approaches for successful implementation.},
  file = {/Users/justingebert/Zotero/storage/S4BEYU4N/Ugwueze and Chukwunweike - 2024 - Continuous Integration and Deployment Strategies for Streamlined DevOps in Software Engineering and.pdf}
}

@misc{wangEmpiricalResearchUtilizing2025,
  title = {Empirical {{Research}} on {{Utilizing LLM-based Agents}} for {{Automated Bug Fixing}} via {{LangGraph}}},
  author = {Wang, Jialin and Duan, Zhihua},
  year = {2025},
  month = jan,
  publisher = {Computer Science},
  doi = {10.33774/coe-2025-jbpg6},
  urldate = {2025-03-12},
  abstract = {This paper presents a novel framework for automated code generation and debugging, designed to improve accuracy, efficiency, and scalability in software development. The proposed system integrates three core components---LangGraph, GLM-4-Flash, and ChromaDB---within a four-step iterative workflow to deliver robust performance and seamless functionality.},
  archiveprefix = {Computer Science},
  copyright = {https://www.cambridge.org/engage/coe/legal-information?show=terms-of-use},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/PAEXZG5F/Wang and Duan - 2025 - Empirical Research on Utilizing LLM-based Agents for Automated Bug Fixing via LangGraph.pdf}
}

@misc{xiaAgentlessDemystifyingLLMbased2024,
  title = {Agentless: {{Demystifying LLM-based Software Engineering Agents}}},
  shorttitle = {Agentless},
  author = {Xia, Chunqiu Steven and Deng, Yinlin and Dunn, Soren and Zhang, Lingming},
  year = {2024},
  month = oct,
  number = {arXiv:2407.01489},
  eprint = {2407.01489},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.01489},
  urldate = {2025-04-24},
  abstract = {Recent advancements in large language models (LLMs) have significantly advanced the automation of software development tasks, including code synthesis, program repair, and test generation. More recently, researchers and industry practitioners have developed various autonomous LLM agents to perform end-to-end software development tasks. These agents are equipped with the ability to use tools, run commands, observe feedback from the environment, and plan for future actions. However, the complexity of these agent-based approaches, together with the limited abilities of current LLMs, raises the following question: Do we really have to employ complex autonomous software agents? To attempt to answer this question, we build Agentless -- an agentless approach to automatically solve software development problems. Compared to the verbose and complex setup of agent-based approaches, Agentless employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools. Our results on the popular SWE-bench Lite benchmark show that surprisingly the simplistic Agentless is able to achieve both the highest performance (32.00\%, 96 correct fixes) and low cost (\$0.70) compared with all existing open-source software agents! Furthermore, we manually classified the problems in SWE-bench Lite and found problems with exact ground truth patch or insufficient/misleading issue descriptions. As such, we construct SWE-bench Lite-S by excluding such problematic issues to perform more rigorous evaluation and comparison. Our work highlights the current overlooked potential of a simple, interpretable technique in autonomous software development. We hope Agentless will help reset the baseline, starting point, and horizon for autonomous software agents, and inspire future work along this crucial direction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/2IPAZLQS/Xia et al. - 2024 - Agentless Demystifying LLM-based Software Engineering Agents.pdf;/Users/justingebert/Zotero/storage/9CAYV9Y4/2407.html}
}

@inproceedings{xiaAutomatedProgramRepair2023,
  title = {Automated {{Program Repair}} in the {{Era}} of {{Large Pre-trained Language Models}}},
  booktitle = {2023 {{IEEE}}/{{ACM}} 45th {{International Conference}} on {{Software Engineering}} ({{ICSE}})},
  author = {Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  year = {2023},
  month = may,
  pages = {1482--1494},
  issn = {1558-1225},
  doi = {10.1109/ICSE48619.2023.00129},
  urldate = {2025-05-19},
  abstract = {Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.},
  keywords = {Automated Program Repair,Codes,Computer bugs,Distance measurement,Faces,Machine Learning,Maintenance engineering,Software,Task analysis},
  file = {/Users/justingebert/Zotero/storage/WPX5SX7L/Xia et al. - 2023 - Automated Program Repair in the Era of Large Pre-trained Language Models.pdf}
}

@inproceedings{xiaAutomatedProgramRepair2024,
  title = {Automated {{Program Repair}} via {{Conversation}}: {{Fixing}} 162 out of 337 {{Bugs}} for \$0.42 {{Each}} Using {{ChatGPT}}},
  shorttitle = {Automated {{Program Repair}} via {{Conversation}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Xia, Chunqiu Steven and Zhang, Lingming},
  year = {2024},
  month = sep,
  pages = {819--831},
  publisher = {ACM},
  address = {Vienna Austria},
  doi = {10.1145/3650212.3680323},
  urldate = {2025-05-12},
  isbn = {979-8-4007-0612-7},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5VD7NFLZ/Xia and Zhang - 2024 - Automated Program Repair via Conversation Fixing 162 out of 337 Bugs for $0.42 Each using ChatGPT.pdf}
}

@misc{yangSWEagentAgentComputerInterfaces2024,
  title = {{{SWE-agent}}: {{Agent-Computer Interfaces Enable Automated Software Engineering}}},
  shorttitle = {{{SWE-agent}}},
  author = {Yang, John and Jimenez, Carlos E. and Wettig, Alexander and Lieret, Kilian and Yao, Shunyu and Narasimhan, Karthik and Press, Ofir},
  year = {2024},
  month = nov,
  number = {arXiv:2405.15793},
  eprint = {2405.15793},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15793},
  urldate = {2025-04-20},
  abstract = {Language model (LM) agents are increasingly being used to automate complicated tasks in digital environments. Just as humans benefit from powerful software applications, such as integrated development environments, for complex tasks like software engineering, we posit that LM agents represent a new category of end users with their own needs and abilities, and would benefit from specially-built interfaces to the software they use. We investigate how interface design affects the performance of language model agents. As a result of this exploration, we introduce SWE-agent: a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs. We evaluate SWE-agent on SWE-bench and HumanEvalFix, achieving state-of-the-art performance on both with a pass@1 rate of 12.5\% and 87.7\%, respectively, far exceeding the previous state-of-the-art achieved with non-interactive LMs. Finally, we provide insight on how the design of the ACI can impact agents' behavior and performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/CE2TI6N9/Yang et al. - 2024 - SWE-agent Agent-Computer Interfaces Enable Automated Software Engineering.pdf;/Users/justingebert/Zotero/storage/V37ZJVGM/2405.html}
}

@article{yaoREACSYNERGIZINGREASONING2023,
  title = {{{REAC T}}: {{SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/YGV27J2I/Yao et al. - 2023 - REAC T SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS.pdf}
}

@misc{yaoReActSynergizingReasoning2023,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2025-04-22},
  abstract = {While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/justingebert/Zotero/storage/XVETJQEI/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf}
}

@inproceedings{yinThinkRepairSelfDirectedAutomated2024,
  title = {{{ThinkRepair}}: {{Self-Directed Automated Program Repair}}},
  shorttitle = {{{ThinkRepair}}},
  booktitle = {Proceedings of the 33rd {{ACM SIGSOFT International Symposium}} on {{Software Testing}} and {{Analysis}}},
  author = {Yin, Xin and Ni, Chao and Wang, Shaohua and Li, Zhenhao and Zeng, Limin and Yang, Xiaohu},
  year = {2024},
  month = sep,
  pages = {1274--1286},
  publisher = {ACM},
  address = {Vienna Austria},
  doi = {10.1145/3650212.3680359},
  urldate = {2025-03-12},
  isbn = {979-8-4007-0612-7},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/5BBJV22R/Yin et al. - 2024 - ThinkRepair Self-Directed Automated Program Repair.pdf}
}

@article{zhangPATCHEmpoweringLarge2025,
  title = {{{PATCH}}: {{Empowering Large Language Model}} with {{Programmer-Intent Guidance}} and {{Collaborative-Behavior Simulation}} for {{Automatic Bug Fixing}}},
  shorttitle = {{{PATCH}}},
  author = {Zhang, Yuwei and Jin, Zhi and Xing, Ying and Li, Ge and Liu, Fang and Zhu, Jiaxin and Dou, Wensheng and Wei, Jun},
  year = {2025},
  month = feb,
  journal = {ACM Transactions on Software Engineering and Methodology},
  pages = {3718739},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3718739},
  urldate = {2025-03-24},
  abstract = {Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.},
  langid = {english},
  file = {/Users/justingebert/Zotero/storage/VFST3JBC/Zhang et al. - 2025 - PATCH Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Si.pdf}
}

@article{zhangSTEAMSimulatingInTeractive2025,
  title = {{{STEAM}}: {{Simulating}} the {{InTeractive BEhavior}} of {{ProgrAMmers}} for {{Automatic Bug Fixing}}},
  shorttitle = {{{STEAM}}},
  author = {Zhang, Yuwei and Jin, Zhi and Xing, Ying and Li, Ge},
  year = {2025},
  month = feb,
  journal = {ACM Transactions on Software Engineering and Methodology},
  eprint = {2308.14460},
  primaryclass = {cs},
  pages = {3718739},
  issn = {1049-331X, 1557-7392},
  doi = {10.1145/3718739},
  urldate = {2025-03-06},
  abstract = {Bug fixing holds significant importance in software development and maintenance. Recent research has made notable progress in exploring the potential of large language models (LLMs) for automatic bug fixing. However, existing studies often overlook the collaborative nature of bug resolution, treating it as a single-stage process. To overcome this limitation, we introduce a novel stage-wise framework named STEAM in this paper. The objective of STEAM is to simulate the interactive behavior of multiple programmers involved in various stages across the bug's life cycle. Taking inspiration from bug management practices, we decompose the bug fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to imitate the collaborative abilities of programmers during the resolution of software bugs. By harnessing the collective contribution, STEAM effectively enhances the bug-fixing capabilities of LLMs. We implement STEAM by employing the powerful dialogue-based LLM -- ChatGPT. Our evaluation on the widely adopted bug-fixing benchmark demonstrates that STEAM has achieved a new state-of-the-art level of bug-fixing performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering},
  file = {/Users/justingebert/Zotero/storage/VLVR2KIN/Zhang et al. - 2025 - STEAM Simulating the InTeractive BEhavior of ProgrAMmers for Automatic Bug Fixing.pdf;/Users/justingebert/Zotero/storage/YPJKESGG/2308.html}
}
